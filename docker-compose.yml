services:

  ###############################
  #  BATCH LAYER (HDFS)
  ###############################
  
  namenode:
    image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8
    platform: linux/amd64
    container_name: namenode
    hostname: namenode
    volumes:
      - hadoop_namenode:/hadoop/dfs/name
    environment:
      - CLUSTER_NAME=hadoop-cluster
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
      - HDFS_CONF_dfs_replication=1
      - HDFS_CONF_dfs_datanode_use_datanode_hostname=true
      - HDFS_CONF_dfs_client_use_datanode_hostname=true 
    ports:
      - "9870:9870"
      - "9000:9000"
    networks:
      - lambda-net
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9870/webhdfs/v1/?op=GETFILESTATUS"]
      interval: 10s
      timeout: 10s
      retries: 5
      start_period: 30s
    deploy:
      resources:
        limits:
          cpus: '1.5'

  datanode:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    platform: linux/amd64
    container_name: datanode
    hostname: datanode
    volumes:
      - hadoop_datanode:/hadoop/dfs/data
    environment:
      SERVICE_PRECONDITION: "namenode:9870"
      CORE_CONF_fs_defaultFS: hdfs://namenode:9000
      HDFS_CONF_dfs_replication: 1
      HDFS_CONF_dfs_datanode_use_datanode_hostname: "true" 
      HDFS_CONF_dfs_datanode_address: "0.0.0.0:9866"
      HDFS_CONF_dfs_datanode_ipc_address: "0.0.0.0:9867"
    depends_on:
      namenode:
        condition: service_healthy
    networks:
      - lambda-net
    healthcheck:
      test: ["CMD", "hdfs", "dfsadmin", "-report"]
      interval: 10s
      timeout: 10s
      retries: 5
      start_period: 30s
    deploy:
      resources:
        limits:
          cpus: '1.5'

  ###############################
  #  KAFKA MESSAGE BROKER
  ###############################

  zookeeper:
    image: confluentinc/cp-zookeeper:7.5.0
    platform: linux/amd64
    container_name: zookeeper
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    ports:
      - "2181:2181"
    networks:
      - lambda-net
    healthcheck:
      test: ["CMD", "nc", "-z", "localhost", "2181"]
      interval: 10s
      timeout: 10s
      retries: 5
      start_period: 30s

  kafka:
    image: confluentinc/cp-kafka:7.5.0
    platform: linux/amd64
    container_name: kafka
    depends_on:
      zookeeper:
        condition: service_healthy
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092,PLAINTEXT_HOST://localhost:29092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"
    ports:
      - "9092:9092"
      - "29092:29092"
    mem_limit: 2g
    networks:
      - lambda-net
    healthcheck:
      test: ["CMD", "kafka-topics", "--bootstrap-server", "localhost:9092", "--list"]
      interval: 15s
      timeout: 10s
      retries: 5
      start_period: 60s

  kafka-init:
    image: confluentinc/cp-kafka:7.5.0
    platform: linux/amd64
    container_name: kafka-init
    depends_on:
      kafka:
        condition: service_healthy
    command: >
      bash -c "kafka-topics --bootstrap-server kafka:9092 --create --if-not-exists 
      --topic crypto-prices --partitions 3 --replication-factor 1"
    networks:
      - lambda-net

  ###############################
  #  SPEED LAYER (CASSANDRA)
  ###############################
  
  cassandra-seed:
    image: cassandra:4.1
    platform: linux/amd64
    container_name: cassandra-seed
    restart: on-failure
    ports:
      - "9042:9042"
    volumes:
      - cassandra_data:/var/lib/cassandra
      - ./cassandra-config:/docker-entrypoint-initdb.d 
    environment:
      - CASSANDRA_CLUSTER_NAME=IoTCluster
      - CASSANDRA_SEEDS=cassandra-seed
      - MAX_HEAP_SIZE=1G
      - HEAP_NEWSIZE=256M
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 2G
        reservations:
          cpus: '0.5'
    networks:
      - lambda-net
    healthcheck:
      test: ["CMD", "cqlsh", "-e", "describe keyspaces"]
      interval: 15s
      timeout: 10s
      retries: 10
      start_period: 90s

  ###############################
  #  SERVIZI APPLICATIVI
  ###############################

  init-services:
    build:
      context: ./iot-producer
    container_name: init-services
    command: ["python", "start.py"]
    depends_on:
      cassandra-seed:
        condition: service_healthy
      namenode:
        condition: service_healthy
    networks:
      - lambda-net
    restart: on-failure

  ###############################
  #  KAFKA BASED SERVICES
  ###############################

  kafka-producer:
    build:
      context: ./iot-producer
    container_name: kafka-producer
    command: ["python", "kafka_producer.py"]
    environment:
      - KAFKA_BOOTSTRAP_SERVERS=kafka:9092
      - KAFKA_TOPIC=crypto-prices
      - HDFS_HOST=namenode
    depends_on:
      kafka:
        condition: service_healthy
      init-services:
        condition: service_completed_successfully
    networks:
      - lambda-net
    volumes:
      - producer_buffer:/buffer
    restart: on-failure

  speed-layer-consumer:
    build:
      context: ./kafka-consumers
    container_name: speed-layer-consumer
    command: ["python", "speed_layer_consumer.py"]
    environment:
      - KAFKA_BOOTSTRAP_SERVERS=kafka:9092
      - KAFKA_TOPIC=crypto-prices
      - KAFKA_SPEED_GROUP_ID=speed-layer-group
      - CASSANDRA_HOST=cassandra-seed
      - HDFS_HOST=namenode
    depends_on:
      kafka:
        condition: service_healthy
      cassandra-seed:
        condition: service_healthy
    networks:
      - lambda-net
    restart: on-failure

  batch-layer-consumer:
    build:
      context: ./kafka-consumers
    container_name: batch-layer-consumer
    command: ["python", "batch_layer_consumer.py"]
    environment:
      - KAFKA_BOOTSTRAP_SERVERS=kafka:9092
      - KAFKA_TOPIC=crypto-prices
      - KAFKA_BATCH_GROUP_ID=batch-layer-group
      - HDFS_HOST=namenode
      - BATCH_SIZE=5000
      - BATCH_FLUSH_INTERVAL=180
    depends_on:
      kafka:
        condition: service_healthy
      namenode:
        condition: service_healthy
    networks:
      - lambda-net
    volumes:
      - producer_buffer:/buffer
    restart: on-failure

  ###############################
  #  SPARK CLUSTER
  ###############################

  spark-master:
    image: apache/spark:3.5.0
    container_name: spark-master
    command: /opt/spark/bin/spark-class org.apache.spark.deploy.master.Master
    ports:
      - "8080:8080"
      - "7077:7077"
    volumes:
      - ./spark-jobs:/opt/spark/jobs
    networks:
      - lambda-net
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080"]
      interval: 10s
      timeout: 10s
      retries: 5
      start_period: 30s

  spark-worker-1:
    image: apache/spark:3.5.0
    container_name: spark-worker-1
    command: /opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077
    environment:
      - SPARK_WORKER_MEMORY=1G
      - SPARK_WORKER_CORES=1
    depends_on:
      spark-master:
        condition: service_healthy
    volumes:
      - ./spark-jobs:/opt/spark/jobs
    networks:
      - lambda-net
    deploy:
      resources:
        limits:
          cpus: '1.5'

  spark-worker-2:
    image: apache/spark:3.5.0
    container_name: spark-worker-2
    command: /opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077
    environment:
      - SPARK_WORKER_MEMORY=1G
      - SPARK_WORKER_CORES=1
    depends_on:
      spark-master:
        condition: service_healthy
    volumes:
      - ./spark-jobs:/opt/spark/jobs
    networks:
      - lambda-net
    deploy:
      resources:
        limits:
          cpus: '1.5'

  spark-worker-3:
    image: apache/spark:3.5.0
    container_name: spark-worker-3
    command: /opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077
    environment:
      - SPARK_WORKER_MEMORY=1G
      - SPARK_WORKER_CORES=1
    depends_on:
      spark-master:
        condition: service_healthy
    volumes:
      - ./spark-jobs:/opt/spark/jobs
    networks:
      - lambda-net
    deploy:
      resources:
        limits:
          cpus: '1.5'

  spark-scheduler:
    build:
      context: ./spark-scheduler
    container_name: spark-scheduler
    environment:
      - SPARK_MASTER_URL=spark://spark-master:7077
    depends_on:
      spark-master:
        condition: service_healthy
      namenode:
        condition: service_healthy
    volumes:
      - ./spark-jobs:/opt/spark/jobs
    networks:
      - lambda-net
    restart: on-failure

  ###############################
  #  DASHBOARD
  ###############################

  dashboard:
    build:
      context: ./dashboard
    container_name: dashboard
    ports:
      - "5000:5000"
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
      - ./hadoop-job:/hadoop-job
      - ./dashboard:/app
    environment:
      - FLASK_ENV=development
      - FLASK_DEBUG=1
    depends_on:
      init-services:
        condition: service_completed_successfully
      cassandra-seed:
        condition: service_healthy
      spark-master:
        condition: service_healthy
    networks:
      - lambda-net
    deploy:
      resources:
        reservations:
          cpus: '0.3'

# Definizione della rete virtuale
networks:
  lambda-net:
    driver: bridge

# Definizione dei volumi per la persistenza
volumes:
  hadoop_namenode:
  hadoop_datanode:
  cassandra_data:
  producer_buffer: